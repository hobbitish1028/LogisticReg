---
title: "Intro_to_LogReg"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Intro_to_LogReg}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r,echo = FALSE,include=FALSE}
library("Rcpp")
library(glmnet)
library(dplyr)
library(ggplot2)
Rcpp::sourceCpp('~/Desktop/Courses/625Bigdata/Logistic_package/src/LogRegCpp.cpp')

Logreg<-function(X,y,maxit = 5000){
  n<-dim(X)[1]
  X<-cbind(rep(0,n),X)
  p<-dim(X)[2]
  ### Use rcpp
  result <- LogRegcpp(X,rep(0,p),y,maxit = maxit)
  result$loss <- result$loss[result$loss !=0 ]
  result$prediction <- result$P > 0.5
  result$accuracy <- mean(result$prediction == y)
  
  return(result)
}

My_predict<-function(fit,newx){
  n<-dim(newx)[1]
  X<-cbind(rep(0,n),newx)
  p<-dim(X)[2]
  result <- X%*%fit$x
  return( as.numeric(result>0))
}

```


## Generating binomial data
The model is Gaussian mixture model:

There are two groups, for sample j with label i (i can be 0 or 1), the distribution is 
$$X_j^i \sim N(\mu_i,\sigma^2)$$ 
The following is an toy example with 20000 samples and 100 features (we use it as training model). Among them, 10000 samples are labeled with 0 and the rest are labeled as 1. And the testing model follows the same design as training model.


```{r}

sigma<-4
set.seed(123)
n<-1e4
p<-1e2
mu1<-rnorm(p)
mu2<-rnorm(p)
X1<-matrix(mu1+rnorm(n*p,0,sigma),n,p,byrow = TRUE)
X2<-matrix(mu2+rnorm(n*p,0,sigma),n,p,byrow = TRUE)
### Train data
X<-rbind(X1,X2)
y<-rep(c(1,0),each=n)
### Test data
test_x<-rbind( matrix(mu1+rnorm(n*p,0,sigma),n,p,byrow = TRUE), 
               matrix(mu2+rnorm(n*p,0,sigma),n,p,byrow = TRUE)  )
test_y<-rep(c(1,0),each=n)

```


## How to use LogReg Function
Under most of the condition, we only need to input the n by p data matrix $X_{n,p}$ and binomial result $Y_n$ (whose value is 0 or 1). 



```{r}
fit<-Logreg(X,y)
```

We can judge the convergence of algorithm by plotting the loss function. My function will judge by itself and stop when it converges. And the default maximal iteration number is 5000. In the application, if the result diverge according to the plot, we can tune the parameter maxit until it converges, which is important to achieve a great prediction. (As for the optimization detail, we use adam-like algorithm, which is quite stable and converges quickly. Different from SGD, it is less sensitive to parameter, and the default parameter can deal with most of the condition.)

```{r}
plot(fit$loss,main = "Convergence of the result",xlab = "iteration",ylab = "-loglikelihood")

plot(fit$P[c(1:100,1e4+(1:100))],main="Probability Distribution (Prediction)",xlab = "sample number",ylab = "probability of group 1")
```



##Accuracy of the training data and testing data
```{r}
my_prediction<-My_predict(fit,newx = test_x)
```

### Accuracy of the training data 
```{r}
fit$accuracy
```

### Accuracy of the testing data
```{r}
mean(my_prediction == test_y)
```


## GLMNET Package
Glmnet is famous package, which use fortran to speed up the calculation, and it values as a frequently used package for statistican. But when dealing with big data, we can't judge the convergence of it, thus we may get a less satisfying result.

```{r}
t1<-proc.time()
fit0<-glmnet(X,y,family = "binomial")
result2<- predict(fit0, newx = X,  type = "class")
proc.time()-t1
```

### Accuracy of the training data (glmnet)
```{r}
mean(result2==y)
```

### Accuracy of the testing data (glmnet)
```{r}
result2<- predict(fit0, newx = test_x,  type = "class")
mean(result2==test_y)
```


## CV.glmnet

Cv.glmnet is a very strong function with great maturity. It uses lasso & ridge penalty to improve the result and avoid overfitting, and cross validation is introduced to avoid the trouble of tuning. However, it may thus cost much more time as tradeoff.

```{r}
t1<-proc.time()
fit0<-cv.glmnet(X,y,family = "binomial")
result3<- predict(fit0, newx = X,  type = "class")
proc.time()-t1
```

### Accuracy of the training data (cv.glmnet, with cross validation and lasso penalty)
```{r}
mean(result2==y)
```

### Accuracy of the testing data (cv.glmnet, with cross validation and lasso penalty)
```{r}
result2<- predict(fit0, newx = test_x,  type = "class")
mean(result2==test_y)
```


## Comparison between three method
```{r,echo=FALSE}

class <- rep(c("train","test"),3)
accuracy<-c(0.95585,0.9564,0.9208,0.92072,0.9207,0.95575)
method<- rep(c("My_LogReg","glmnet","cv.glmnet"),each=2)
dat<-data.frame(class,accuracy,method)

ggplot(
  dat %>%
    filter(
      class %in% c("train","test")
    ) %>% group_by(method,class) %>% summarize(
      acc = accuracy
    )
)+ aes(
  x=method,
  y=acc)+
  labs(
    x="Method",
    y="Accuracy"
  )+ geom_col(
    aes(fill=factor(class)),
    position='dodge'
  )+coord_cartesian( ylim = c(0.90, 0.97))+ggtitle("Comparison between three models") +
  theme(plot.title = element_text(hjust = 0.5))
```

|    	|	My LogReg |Glmnet|cv.glmnet|
|  ----  		| ----  |---|-----|
| time   	| 5.37 | 2.17 | 19.31|

We can infer from the plot that glmnet may fail to converge under the condition of big data, which may result from its stopping before convergence (which also explains why it takes less time).

With lasso penalty and cross validation, cv.glmnet doesn't overfit, and its accuracy of test data is even higher than that of the train data. But it is very time-expensive.

My Logistic regression doesn't overfit, either. We may attribute it to the superiority of optimization algorithm (which is stable and likely to converge to global minimal rather than local minimal, thus avoiding overfitting). Besides, because it is adam-like algorithm, it converges quickly and thus needs fewer iteration. So it converges quickly.

